{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "le_net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carvalheirafc/deep_learning_stuff/blob/master/neural_networks/le_net/le_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "htyq6krzTkip"
      },
      "source": [
        "## Import Section\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxyYVKgGtUZH",
        "outputId": "6ab71204-1fd7-4ea3-ba4f-5b9b39281dee",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.utils import np_utils\n",
        "from keras import backend\n",
        "\n",
        "from imutils import paths\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MY8JLHdpvUDA"
      },
      "source": [
        "## LeNet Class Definition![alt text](https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WObRC7NqhL2z",
        "colab": {}
      },
      "source": [
        "class LeNet:\n",
        "  @staticmethod\n",
        "  def build(n_channels, \n",
        "            rows, \n",
        "            cols, \n",
        "            n_classes, \n",
        "            activation='relu', \n",
        "            weights_path=None):\n",
        "    model = Sequential()\n",
        "    \n",
        "    input_Shape = (rows, cols, n_channels)\n",
        "    \n",
        "   \n",
        "    #Convolution and Pooling Layers\n",
        "    model.add(Conv2D(20, kernel_size=5, activation=activation, input_shape=input_Shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(50, kernel_size=5, activation=activation))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    \n",
        "    #Flatten and Fully Conected layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation(activation))\n",
        "    \n",
        "    # OutputLayer SOFTMAX activation\n",
        "    model.add(Dense(n_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    # if a weights path is supplied (inicating that the model was\n",
        "    # pre-trained), then load the weights\n",
        "    if weights_path is not None:\n",
        "      model.load_weights(weights_path)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE_ez5oZqMSm"
      },
      "source": [
        "## Reading or Import Image Files.\n",
        "São Criados dois arrays, data para guardar todos os dados de entrada e labels para todas as labels.\n",
        "\n",
        "- Listar todos os arquivos de imagem de todos os sub-diretórios.\n",
        "- Percorrer a lista e fazer a leitura do arquivo de imagem.\n",
        "  - convertida para modo em grayscale\n",
        "  - Convertida para o formato de NumpyArray\n",
        "  - Adicionada ao fim do vetor com todos os dados.\n",
        "  \n",
        "- Label é extraida usando o caminho da imagem.\n",
        " - Ex: root_path/Letras/0/img1.png \n",
        " - Vai gerar a label: 0, que corresponde a letra A.\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQYZCiZVrvJs",
        "outputId": "06009f0a-206f-4a53-de66-e98819197bf9",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "root_path = 'Letras/'\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "all_images_paths = list(paths.list_images(root_path))\n",
        "current = -1\n",
        "\n",
        "\n",
        "try:\n",
        "    for image_path in all_images_paths:\n",
        "    \n",
        "    \n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = img_to_array(image)\n",
        "        data.append(image)\n",
        "    \n",
        "        label = image_path.split(os.path.sep)[-2][7:]\n",
        "        label = int(label)\n",
        "        labels.append(label)\n",
        "        \n",
        "        if current != label:\n",
        "            print('Loading Files into arrays...')\n",
        "            print('Loading Class [{}]'.format(label))\n",
        "            current = label\n",
        "        \n",
        "        current = label\n",
        "    print('All Files are Loaded Successfully...')\n",
        "\n",
        "except IOError:\n",
        "    print('Something went Wrong loading the files')    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Files into arrays...\n",
            "Loading Class [0]\n",
            "Loading Files into arrays...\n",
            "Loading Class [1]\n",
            "Loading Files into arrays...\n",
            "Loading Class [10]\n",
            "Loading Files into arrays...\n",
            "Loading Class [11]\n",
            "Loading Files into arrays...\n",
            "Loading Class [12]\n",
            "Loading Files into arrays...\n",
            "Loading Class [13]\n",
            "Loading Files into arrays...\n",
            "Loading Class [14]\n",
            "Loading Files into arrays...\n",
            "Loading Class [15]\n",
            "Loading Files into arrays...\n",
            "Loading Class [16]\n",
            "Loading Files into arrays...\n",
            "Loading Class [17]\n",
            "Loading Files into arrays...\n",
            "Loading Class [18]\n",
            "Loading Files into arrays...\n",
            "Loading Class [19]\n",
            "Loading Files into arrays...\n",
            "Loading Class [2]\n",
            "Loading Files into arrays...\n",
            "Loading Class [20]\n",
            "Loading Files into arrays...\n",
            "Loading Class [21]\n",
            "Loading Files into arrays...\n",
            "Loading Class [22]\n",
            "Loading Files into arrays...\n",
            "Loading Class [23]\n",
            "Loading Files into arrays...\n",
            "Loading Class [24]\n",
            "Loading Files into arrays...\n",
            "Loading Class [25]\n",
            "Loading Files into arrays...\n",
            "Loading Class [3]\n",
            "Loading Files into arrays...\n",
            "Loading Class [4]\n",
            "Loading Files into arrays...\n",
            "Loading Class [5]\n",
            "Loading Files into arrays...\n",
            "Loading Class [6]\n",
            "Loading Files into arrays...\n",
            "Loading Class [7]\n",
            "Loading Files into arrays...\n",
            "Loading Class [8]\n",
            "Loading Files into arrays...\n",
            "Loading Class [9]\n",
            "All Files are Loaded Successfully...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_Ii5CSETgEc",
        "colab": {}
      },
      "source": [
        "save_files = False\n",
        "\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "if save_files:\n",
        "    try:\n",
        "        np.save('char_input_data', data)\n",
        "        np.save('labels', labels)\n",
        "    except IOError:\n",
        "        print('Error while saving the files')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nsVOUgHST5zM"
      },
      "source": [
        "## Model Configuration and Train Run\n",
        "\n",
        "Alguns parâmetros usados no trein:\n",
        "\n",
        "- Epochs: 10\n",
        "\n",
        "- Learning Rate: 1e-3\n",
        "\n",
        "- Batch Size: 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BermO3bQTgEf",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "sess = tf.Session(config=config) \n",
        "backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhGXgNoTVTbe",
        "colab_type": "text"
      },
      "source": [
        "### Divisão dos conjuntos de treino e teste\n",
        "\n",
        "#### Como já visto em sala de aula, os conjuntos devem ser de tamanhos semelhantes e totalmente disjuntos.\n",
        "\n",
        "Logo Foi usada a função **train_test_split** para dividir corretamente a base de dados e labels nos respectivos conjuntos.\n",
        "- Foi definido que cada conjunto teria cerca de 1/3 do tamanho total da base de dados. \n",
        "- Seed pseudo-randomica passada para garantir uma particionalização diferente a cada execução do código.\n",
        "\n",
        "- Os conjuntos de de treino e teste das labels foram transformadas em arrays categóricos. Ou seja:\n",
        "- Classe **0** = **array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)**\n",
        "\n",
        "- Classe **1** = **array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)**\n",
        "\n",
        "  ...\n",
        "- Classe **25** =  **array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7hVsWQSjZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = random.randint(1, 999)\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=seed)\n",
        "cat_y_train = np_utils.to_categorical(y_train, 26)\n",
        "cat_y_test = np_utils.to_categorical(y_test, 26)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNGThK5LTgEh",
        "colab": {}
      },
      "source": [
        "n_epochs = 10\n",
        "ini_learning_rate = 1e-3\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWXSwXAKXbXs",
        "colab_type": "text"
      },
      "source": [
        "### Test Section\n",
        "\n",
        "- Modelo é construído usando as seguintes configurações:\n",
        "  - n_channels = 1, pois a imagem só possui um canal(greyscale).\n",
        "  - rows(height) = 28, largura da imagem.\n",
        "  - cols(width) = 28, largura da imagem.\n",
        "  - n_classes = 26, cada letra do alfabeto representada discretamente entre 0 ~ 25\n",
        "  - activation = relu, Rectified Linear Activation Unit\n",
        "```\n",
        "if input > 0:\n",
        "  return input\n",
        "  else:\n",
        "\treturn 0\n",
        "```\n",
        "  - weights_path=none, Inicialmente não temos nenhum conjunto de pesos para passar a rede neural.\n",
        " \n",
        "- Otimizador **Adam**:\n",
        "    - Diferentemente dos otimizadores convencionais que possuem um learning_rate fixo para todos os pesos e atualizações, o Adam permite uma taxa de aprendizado inicial para a rede e permite também a adaptação dessa ao longo do processo de aprendizagem.\n",
        "    - lerning_rate = 1e-3\n",
        "    - decay(decaimento da taxa de learning_rate) = 1e-3/10\n",
        "    \n",
        " - Modelo é compilado com:\n",
        "  - loss='categorical_crossentropy', usado por conta do problema de reconhecimento ser muit-classes.\n",
        "  - metrics='categorical_accuracy'\n",
        "  \n",
        " - A cada epoch é feita uma avaliação tendo como base a métrica **categorical_accuracy** e caso haja alguma melhora no score, um novo arquivo é salvo contendo os pesos da rede neural.\n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC1qozzTSjZN",
        "colab_type": "code",
        "colab": {},
        "outputId": "62ac094f-da43-436c-b76e-887cef39cc4a"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    train_model = LeNet.build(n_channels=1, \n",
        "                              rows=28,\n",
        "                              cols=28, \n",
        "                              n_classes=26, \n",
        "                              activation='relu', \n",
        "                              weights_path=None)\n",
        "\n",
        "    opt = Adam(lr=ini_learning_rate, decay=ini_learning_rate / n_epochs)\n",
        "    train_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[categorical_accuracy])\n",
        "    try:\n",
        "        file_path = 'weights_Letras.h5'\n",
        "        check_point = ModelCheckpoint(file_path, \n",
        "                                      monitor='categorical_accuracy', \n",
        "                                      verbose=0, \n",
        "                                      save_best_only=True,\n",
        "                                      mode='max',\n",
        "                                      save_weights_only=True)\n",
        "        callbacks_list = [check_point]\n",
        "        \n",
        "        train_history = train_model.fit(x=x_train, \n",
        "                                        y=cat_y_train, \n",
        "                                        batch_size=batch_size, \n",
        "                                        epochs=n_epochs,\n",
        "                                        callbacks=callbacks_list,\n",
        "                                        verbose=1)\n",
        "    \n",
        "   \n",
        "    except IOError:\n",
        "        print('Error while saving the Model weights')\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14649/14649 [==============================] - 4s 256us/step - loss: 1.1561 - acc: 0.6705\n",
            "Epoch 2/10\n",
            "14649/14649 [==============================] - 2s 135us/step - loss: 0.2523 - acc: 0.9236\n",
            "Epoch 3/10\n",
            "14649/14649 [==============================] - 2s 144us/step - loss: 0.1801 - acc: 0.9469\n",
            "Epoch 4/10\n",
            "14649/14649 [==============================] - 2s 150us/step - loss: 0.1500 - acc: 0.9546\n",
            "Epoch 5/10\n",
            "14649/14649 [==============================] - 2s 154us/step - loss: 0.1252 - acc: 0.9623\n",
            "Epoch 6/10\n",
            "14649/14649 [==============================] - 2s 137us/step - loss: 0.1085 - acc: 0.9681\n",
            "Epoch 7/10\n",
            "14649/14649 [==============================] - 2s 139us/step - loss: 0.0995 - acc: 0.9697\n",
            "Epoch 8/10\n",
            "14649/14649 [==============================] - 2s 155us/step - loss: 0.0855 - acc: 0.9743\n",
            "Epoch 9/10\n",
            "14649/14649 [==============================] - 2s 135us/step - loss: 0.0794 - acc: 0.9740\n",
            "Epoch 10/10\n",
            "14649/14649 [==============================] - 2s 146us/step - loss: 0.0693 - acc: 0.9775\n",
            "File Writed as: weights_Letras.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KkLKlj8nX2K",
        "colab_type": "text"
      },
      "source": [
        "### Test Run\n",
        "\n",
        "- Um novo modelo é criado com agora os pesos da rede neural gerada na fase treino.\n",
        "- Esse novo modelo em teoria possui todo o conhecimento gerado pelo modelo de treino.\n",
        "- Um vetor contendo as predições é gerado para que possa ser usado no cálculo das métricas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFijjRcPSjZQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "93e0bfcc-fba9-43b3-8a95-1f3c7451db22"
      },
      "source": [
        "save = True\n",
        "with tf.Session() as sess:\n",
        "    test_model = LeNet.build(n_channels=1, \n",
        "                             rows=28,\n",
        "                             cols=28, \n",
        "                             n_classes=26, \n",
        "                             activation='relu', \n",
        "                             weights_path=weights_file)\n",
        "\n",
        "    pred = test_model.predict(x_test, verbose=1)\n",
        "sess.close()\n",
        "\n",
        "if save:\n",
        "    np.save('predictions', pred.argmax(1))\n",
        "    np.save('y_test', cat_y_test.argmax(1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6279/6279 [==============================] - 0s 75us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svYeji9JSjZT",
        "colab_type": "code",
        "colab": {},
        "outputId": "f737609b-89ed-4a8b-b20b-45e6aeaabf44"
      },
      "source": [
        "precision, recall, f_betta, support = precision_recall_fscore_support(cat_y_test.argmax(1), \n",
        "                                                                      pred.argmax(1),\n",
        "                                                                      average='weighted')\n",
        "\n",
        "accuracy = accuracy_score(cat_y_test.argmax(1), pred.argmax(1))\n",
        "\n",
        "print('Accuracy: {}'.format(accuracy))\n",
        "print('Precision: {}'.format(precision))\n",
        "print('Recall: {}'.format(recall))\n",
        "print('F Betta: {}'.format(f_betta))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9705367096671444\n",
            "Precision: 0.9709321779686296\n",
            "Recall: 0.9705367096671444\n",
            "F Betta: 0.9705402443069723\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}