{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/carvalheirafc/deep_learning_stuff/blob/master/neural_networks/le_net/le_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htyq6krzTkip"
   },
   "source": [
    "## Import Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxyYVKgGtUZH",
    "outputId": "6ab71204-1fd7-4ea3-ba4f-5b9b39281dee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras import backend\n",
    "\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY8JLHdpvUDA"
   },
   "source": [
    "## LeNet Class Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WObRC7NqhL2z"
   },
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "  @staticmethod\n",
    "  def build(n_channels, \n",
    "            rows, \n",
    "            cols, \n",
    "            n_classes, \n",
    "            activation='relu', \n",
    "            weights_path=None):\n",
    "    model = Sequential()\n",
    "    \n",
    "    input_Shape = (rows, cols, n_channels)\n",
    "    \n",
    "   \n",
    "    #Convolution and Pooling Layers\n",
    "    model.add(Conv2D(20, kernel_size=5, activation=activation, input_shape=input_Shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(50, kernel_size=5, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    #Flatten and Fully Conected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    # OutputLayer SOFTMAX activation\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # if a weights path is supplied (inicating that the model was\n",
    "    # pre-trained), then load the weights\n",
    "    if weights_path is not None:\n",
    "      model.load_weights(weights_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE_ez5oZqMSm"
   },
   "source": [
    "## Reading or Import Image Files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQYZCiZVrvJs",
    "outputId": "06009f0a-206f-4a53-de66-e98819197bf9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Files into arrays...\n",
      "Loading Class [0]\n",
      "Loading Files into arrays...\n",
      "Loading Class [1]\n",
      "Loading Files into arrays...\n",
      "Loading Class [10]\n",
      "Loading Files into arrays...\n",
      "Loading Class [11]\n",
      "Loading Files into arrays...\n",
      "Loading Class [12]\n",
      "Loading Files into arrays...\n",
      "Loading Class [13]\n",
      "Loading Files into arrays...\n",
      "Loading Class [14]\n",
      "Loading Files into arrays...\n",
      "Loading Class [15]\n",
      "Loading Files into arrays...\n",
      "Loading Class [16]\n",
      "Loading Files into arrays...\n",
      "Loading Class [17]\n",
      "Loading Files into arrays...\n",
      "Loading Class [18]\n",
      "Loading Files into arrays...\n",
      "Loading Class [19]\n",
      "Loading Files into arrays...\n",
      "Loading Class [2]\n",
      "Loading Files into arrays...\n",
      "Loading Class [20]\n",
      "Loading Files into arrays...\n",
      "Loading Class [21]\n",
      "Loading Files into arrays...\n",
      "Loading Class [22]\n",
      "Loading Files into arrays...\n",
      "Loading Class [23]\n",
      "Loading Files into arrays...\n",
      "Loading Class [24]\n",
      "Loading Files into arrays...\n",
      "Loading Class [25]\n",
      "Loading Files into arrays...\n",
      "Loading Class [3]\n",
      "Loading Files into arrays...\n",
      "Loading Class [4]\n",
      "Loading Files into arrays...\n",
      "Loading Class [5]\n",
      "Loading Files into arrays...\n",
      "Loading Class [6]\n",
      "Loading Files into arrays...\n",
      "Loading Class [7]\n",
      "Loading Files into arrays...\n",
      "Loading Class [8]\n",
      "Loading Files into arrays...\n",
      "Loading Class [9]\n",
      "All Files are Loaded Successfully...\n"
     ]
    }
   ],
   "source": [
    "root_path = 'Letras/'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "all_images_paths = list(paths.list_images(root_path))\n",
    "current = -1\n",
    "\n",
    "\n",
    "try:\n",
    "    for image_path in all_images_paths:\n",
    "    \n",
    "    \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "    \n",
    "        label = image_path.split(os.path.sep)[-2][7:]\n",
    "        label = int(label)\n",
    "        labels.append(label)\n",
    "        \n",
    "        if current != label:\n",
    "            print('Loading Files into arrays...')\n",
    "            print('Loading Class [{}]'.format(label))\n",
    "            current = label\n",
    "        \n",
    "        current = label\n",
    "    print('All Files are Loaded Successfully...')\n",
    "\n",
    "except IOError:\n",
    "    print('Something went Wrong loading the files')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_Ii5CSETgEc"
   },
   "outputs": [],
   "source": [
    "save_files = False\n",
    "\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "if save_files:\n",
    "    try:\n",
    "        np.save('char_input_data', data)\n",
    "        np.save('labels', labels)\n",
    "    except IOError:\n",
    "        print('Error while saving the files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsVOUgHST5zM"
   },
   "source": [
    "## Model Configuration and Train Ru\n",
    "\n",
    "- Epochs: 10\n",
    "\n",
    "- Learning Rate: 1e-3\n",
    "\n",
    "- Batch Size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BermO3bQTgEf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config=config) \n",
    "backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randint(1, 999)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=seed)\n",
    "cat_y_train = np_utils.to_categorical(y_train, 26)\n",
    "cat_y_test = np_utils.to_categorical(y_test, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNGThK5LTgEh",
    "outputId": "17b123a5-2139-46ca-e727-cd76f4f5d0a1"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "ini_learning_rate = 1e-3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14649/14649 [==============================] - 4s 256us/step - loss: 1.1561 - acc: 0.6705\n",
      "Epoch 2/10\n",
      "14649/14649 [==============================] - 2s 135us/step - loss: 0.2523 - acc: 0.9236\n",
      "Epoch 3/10\n",
      "14649/14649 [==============================] - 2s 144us/step - loss: 0.1801 - acc: 0.9469\n",
      "Epoch 4/10\n",
      "14649/14649 [==============================] - 2s 150us/step - loss: 0.1500 - acc: 0.9546\n",
      "Epoch 5/10\n",
      "14649/14649 [==============================] - 2s 154us/step - loss: 0.1252 - acc: 0.9623\n",
      "Epoch 6/10\n",
      "14649/14649 [==============================] - 2s 137us/step - loss: 0.1085 - acc: 0.9681\n",
      "Epoch 7/10\n",
      "14649/14649 [==============================] - 2s 139us/step - loss: 0.0995 - acc: 0.9697\n",
      "Epoch 8/10\n",
      "14649/14649 [==============================] - 2s 155us/step - loss: 0.0855 - acc: 0.9743\n",
      "Epoch 9/10\n",
      "14649/14649 [==============================] - 2s 135us/step - loss: 0.0794 - acc: 0.9740\n",
      "Epoch 10/10\n",
      "14649/14649 [==============================] - 2s 146us/step - loss: 0.0693 - acc: 0.9775\n",
      "File Writed as: weights_Letras.h5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_model = LeNet.build(n_channels=1, \n",
    "                              rows=28,\n",
    "                              cols=28, \n",
    "                              n_classes=26, \n",
    "                              activation='relu', \n",
    "                              weights_path=None)\n",
    "\n",
    "    opt = Adam(lr=ini_learning_rate, decay=ini_learning_rate / n_epochs)\n",
    "    train_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    train_history = train_model.fit(x=x_train, \n",
    "                                    y=cat_y_train,\n",
    "                                    batch_size=batch_size, \n",
    "                                    epochs=n_epochs,\n",
    "                                    verbose=1)\n",
    "    \n",
    "    weights_file = 'weights_Letras.h5'\n",
    "    try:\n",
    "        train_model.save_weights(weights_file)\n",
    "        print('File Writed as: {}'.format(weights_file))\n",
    "    except IOError:\n",
    "        print('Error while saving the Model weights')\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6279/6279 [==============================] - 0s 75us/step\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "with tf.Session() as sess:\n",
    "    test_model = LeNet.build(n_channels=1, \n",
    "                             rows=28,\n",
    "                             cols=28, \n",
    "                             n_classes=26, \n",
    "                             activation='relu', \n",
    "                             weights_path=weights_file)\n",
    "\n",
    "    pred = test_model.predict(x_test, verbose=1)\n",
    "sess.close()\n",
    "\n",
    "if save:\n",
    "    np.save('predictions', pred.argmax(1))\n",
    "    np.save('y_test', cat_y_test.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9705367096671444\n",
      "Precision: 0.9709321779686296\n",
      "Recall: 0.9705367096671444\n",
      "F Betta: 0.9705402443069723\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f_betta, support = precision_recall_fscore_support(cat_y_test.argmax(1), \n",
    "                                                                      pred.argmax(1),\n",
    "                                                                      average='weighted')\n",
    "\n",
    "accuracy = accuracy_score(cat_y_test.argmax(1), pred.argmax(1))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F Betta: {}'.format(f_betta))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "le_net.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
